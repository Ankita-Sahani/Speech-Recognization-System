# -*- coding: utf-8 -*-
"""Speech User Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qcFttNEA5wBF1rQLaGvGvKEYsriiNiOD
"""

import os
import matplotlib.pyplot as plt
import numpy as np
import librosa
import librosa.display
import pickle

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize


import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Input, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad

"""# Download the Dataset

The CREMA-D dataset can be found in these locations: <br/>
  Kaggle - https://www.kaggle.com/ejlok1/cremad <br/>
  GitHub - https://github.com/CheyneyComputerScience/CREMA-D
"""

# RECOMENDED
# Upload a dataset from Kaggle
# NOTE: upload kaggle.json into content folder first. Instructions:
#   create account on Kaggle
#   kaggle.json is downloaded from:
#   ONLY THEN run this cell
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download -d ejlok1/cremad
! mkdir data
! unzip cremad.zip -d data

"""Specify the directory"""

# REQUIRED - MODIFY
# default: location if used kaggle method
dataset_path = r"/content/data/AudioWAV"

# OPTIONAL
sample_count = sum(len(files) for _, _, files in os.walk(dataset_path))
print("Number of samples: " + str(sample_count))

# OPTIONAL
# mount a gdrive
from google.colab import drive
drive.mount('/content/drive')

"""# OPTIONAL:  Preprocessing Explanation

this section is will outline how the data is preprocessed.

you changle around the values of variables to get a feel of how it changes the output

put an exaple audio file directory here (single .wav file):
"""

audio_file = r'/content/data/AudioWAV/1001_ITH_HAP_XX.wav'

signal, sample_rate = librosa.load(audio_file)

"""An audiofile can be preprocessed in many differnt ways to make it more compatable with machine learning algorithums. In this project audio files are preprocessed into Mel log spectograms.

Audio data looks like this:
"""

plt.plot(signal)
plt.xlabel('sample')
plt.ylabel('amplitude')

number_of_mels = 128 # number of mel bands (y axis)
number_of_hops = 64 # number of hops in a clip (x axis)
frame_size = 512 # the amount of samples used for FFT in a hop
hop_size = frame_size // 2 # the number of frames the program moves - hopping is used such that frames overlap
number_samples_per_clip = (number_of_hops * hop_size) - 1 # how many samples we will get from each clip
sr = sample_rate

print("number of samples per clip: {}".format(number_samples_per_clip))
print("clip duration: {:.2f}s".format(number_samples_per_clip / sr))
print("clips in signal: {}".format(len(signal) // number_samples_per_clip))
print("spectogram resolution x = {}, y = {}".format(number_of_hops, number_of_mels))

"""The signal is broken down into multiple clips using the number_samples_per_clip"""

clips = []
for clip_number in range(0, len(signal) // number_samples_per_clip):
  clip_start_point = clip_number * number_samples_per_clip
  clip_end_point = clip_start_point + number_samples_per_clip
  clip = signal[clip_start_point : clip_end_point]
  clips.append(clip)

print("number of clips: {}".format(len(clips)))
plt.plot(signal)
for line in (x * number_samples_per_clip + number_samples_per_clip for x in range(0, len(signal) // number_samples_per_clip)):
   plt.axvline(line, c = 'red')
plt.title('signal broken into clips')
plt.legend(['signal', 'end of a clip'])

"""We can then turn each of these clips into a mel log spectogram using librosa. Mel log specrograms are well suited for speech data input into covoloutional 2D neural networks."""

selected_clip = 1

clip = clips[selected_clip]
mel_spec = librosa.feature.melspectrogram(clip, sr, None, frame_size, hop_size, n_mels = number_of_mels)
log_mel_spec = librosa.power_to_db(mel_spec)

plt.figure()
plt.title("Mel_spectogram (power)")
librosa.display.specshow(mel_spec)

plt.figure()
plt.title("log Mel spectogram")
librosa.display.specshow(log_mel_spec)

"""# Preprocessing

The filenames have the following naming convention speekerid_sentance_emotion_emotionlevel.wav.
"""

# REQURIED
# see https://github.com/CheyneyComputerScience/CREMA-D/blob/master/README.md for info on meaning
# each part of the filename has its own list

SENTENCES = np.array(['IEO','TIE','IOM','IWW','TAI','MTI','IWL','ITH','DFA','ITS','TSI','WSI'])
EMOTIONS =  np.array(['ANG','DIS','FEA','HAP','NEU','SAD'])
EMOTION_LEVELS =  np.array(['LO','MD','HI','XX'])


# preprocessessing variables #
# see Preporcessing Explanation for more information
sr = 22050
number_of_mels = 128
number_of_hops = 64
frame_size = 512
hop_size = frame_size // 2
number_samples_per_clip = (number_of_hops * hop_size) - 1

"""The spectograms dictionary can be loaded using one of the following metods:
- load from an already created piclkle file
- create from scratch, optionally save as a pickle file for next time
"""

# OPTIONAL - LOAD FROM PICKLE FILE
# if a dictionary of log Mel spectograms has aleready been created and piclked
# unpickle it hear and skip next step
dict_path = r'/content/drive/MyDrive/spectograms.pickle'

with open(dict_path, 'rb') as pickled_dictionary:
  spectograms = pickle.load(pickled_dictionary)

"""A single clip will be broken down into multple uniform clips. These uniform clips will share the same labels as the parent clip. Each child clip's signal data will be tranformed into a mel log spectogram using parameters set at the start of the code block. More information on this process is in the 'Preprosessing Explanation' section."""

# REQUIRED - CREATE FROM SCRATCH - IF NOT LOADED FROM PICKLE FILE
# see Preporcessing Explanation for more information
# may take a long time! save the pickled drive to google drive for faster speeds when reloading notebook

# clips is a dictionary where all keys return parralel arrays
spectograms = {
    'clip_index': [],
    'parent_clip_index': [],
    'child_clip_index': [],
    'log_mel_spectograms': [],
    'speakers': [],
    'sentences': [],
    'emotions': [],
    'emtion_levels': []
}

clip_index = 0
for parent_index, file_dir in enumerate(os.listdir(dataset_path)):

  # break the file name down into componants
  file_data, _ = file_dir.split('.')
  speaker, sentence, emotion, emotion_level = file_data.split('_')

  # discard any files that are not named correctly
  if not np.any(sentence == SENTENCES):
    print(file_dir + " discarded")
    continue
  if not np.any(emotion == EMOTIONS):
    print(file_dir  + " discarded")
    continue
  if not np.any(emotion_level == EMOTION_LEVELS):
    print(file_dir  + " discarded")
    continue

  signal, _ = librosa.load(os.path.join(dataset_path, file_dir))

  for clip_number in range(len(signal) // number_samples_per_clip):
    clip = signal[clip_number * number_samples_per_clip : clip_number * number_samples_per_clip + number_samples_per_clip]
    log_mel_spec = librosa.power_to_db(librosa.feature.melspectrogram(clip, sr, None, frame_size, hop_size, n_mels = number_of_mels))
    spectograms.get('clip_index').append(clip_index)
    spectograms.get('parent_clip_index').append(parent_index)
    spectograms.get('child_clip_index').append(clip_number)
    spectograms.get('log_mel_spectograms').append(log_mel_spec)
    spectograms.get('speakers').append(speaker)
    spectograms.get('sentences').append(sentence)
    spectograms.get('emotions').append(emotion)
    spectograms.get('emtion_levels').append(emotion_level)

    clip_index = clip_index + 1

# OPTIONAL - SAVE PICKLE FILE FOR NEXT TIME (MUCH FASTER)
# pickle the dictionary such that it can be loaded quickly next time!
dict_output_path = r'/content/drive/MyDrive/spectograms.pickle' # default: saved into mounted google drive

with open(dict_output_path, 'wb') as out_file:
  pickle.dump(spectograms, out_file)

# REQURIED
def get_spectogram_data(index):
    """Function returns a dictionary containgn only the data from a single
    index of spectograms paralelle array dictionary"""
    spectogram = {}
    for key in spectograms.keys():
      dic2[key] = spectograms.get(key)[index]
    return spectogram

"""Here is how the spectograms are accessed

"""

# OPTIONAL
example_index = 30

plt.title("part {} of clip {} where speaker {} says {} while {} at level {}".format(
                                                          spectograms.get('child_clip_index')[example_index],
                                                          spectograms.get('parent_clip_index')[example_index],
                                                          spectograms.get('speakers')[example_index],
                                                          spectograms.get('sentences')[example_index],
                                                          spectograms.get('emotions')[example_index],
                                                          spectograms.get('emtion_levels')[example_index]))
librosa.display.specshow(spectograms.get('log_mel_spectograms')[example_index])

# REQUIRED
def get_new_model():
  optimizer = Adam(0.0001)
  reg = keras.regularizers.l2(0.001)
  activation = "relu"
  dropout = .1
  base = 32

  model = Sequential()

  model.add(Input((number_of_mels, number_of_hops, 1)))

  model.add(Conv2D(base*2**0, (3,3), 1, activation=activation, kernel_regularizer=reg))
  model.add(MaxPool2D(2, 2))
  model.add(Dropout(dropout))


  model.add(Conv2D(base*2**1, (3,3), 1, activation=activation, kernel_regularizer=reg))
  model.add(MaxPool2D(2, 2))
  model.add(Dropout(dropout))


  model.add(Conv2D(base*2**2, (3,3), 1, activation=activation, kernel_regularizer=reg))
  model.add(MaxPool2D(2, 2))
  model.add(Dropout(dropout))


  model.add(Conv2D(base*2**3, (3,3), 1, activation=activation, kernel_regularizer=reg))
  model.add(MaxPool2D(2, 2))
  model.add(Dropout(dropout))


  model.add(Flatten())
  # model.add(Dense(32, activation = activation, kernel_regularizer=reg))
  # model.add(Dropout(.1))
  # model.add(Dense(64, activation = activation, kernel_regularizer=reg))
  # model.add(Dropout(.1))

  model.add(Dense(6, activation='softmax'))

  model.compile(optimizer=optimizer, loss = "categorical_crossentropy", metrics = ["accuracy"])
  return model

"""# Training and Testing the Model Using Normal Train Test Split

The X data is the raw data while the y data is the label of the data. <br/>

The spectograms are converted to a format more friendly to work with. Labels are converted into catgoral or one hot encoded format such that it can be inputed into the keras model.
"""

# REQUIRED
# the key of the spectograms dict to use as y data
y_data_key = 'emotions'
# the array of the y data's string classes can be used to covert the
# label to a numerical value by finding the pos of a label in the array
y_classes_array = EMOTIONS

def prepare_x(xdata):
  xdata = np.array(xdata)
  return xdata.reshape(-1, number_of_mels, number_of_hops, 1)

def prepare_y(ydata_index):
  labels_ohe = []
  labels_numerical = []
  for label_index in ydata_index:
    # e.g: 'HAP' -> 3 -> [0,0,0,1,0,0]
    label_txt = spectograms.get(y_data_key)[label_index]
    label_numerical = np.where(label_txt == y_classes_array)[0][0]
    label_ohe = keras.utils.to_categorical(label_numerical, len(y_classes_array))

    labels_ohe.append(label_ohe)
    labels_numerical.append(label_numerical)
  return np.array(labels_ohe), np.array(labels_numerical)

train_X, test_X, train_y_index, test_y_index = train_test_split(spectograms.get('log_mel_spectograms'),
                                                                spectograms.get('clip_index'))

train_X = prepare_x(train_X)
test_X = prepare_x(test_X)

# parallel to ..._y_index
train_y_ohe, train_y_numerical = prepare_y(train_y_index)
test_y_ohe, test_y_numerical = prepare_y(test_y_index)

print("train_X shape: {}".format(train_X.shape))
print("test_X shape: {}".format(test_X.shape))
print("train_y shape: {}".format(train_y_ohe.shape))
print("test_y shape: {}".format(test_y_ohe.shape))

print("\nlabel -> numerical -> ohe")
for i in range(5):
  print("train_y index {}: {} -> {} -> {}".format(i,
                                                  y_classes_array[train_y_numerical[i]],
                                                  train_y_numerical[i],
                                                  train_y_ohe[i]
                                                  ))

# REQUIRED
batch_size = 128
epochs = 20

model = get_new_model()
model_history = model.fit(train_X, train_y_ohe, batch_size=batch_size, epochs=epochs, validation_data=(test_X, test_y_ohe))

# OPTIONAL
plt.figure()
plt.title('Model Accuracy History')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(model_history.history.get('accuracy'))
plt.plot(model_history.history.get('val_accuracy'))
plt.legend(['accuracy on seen before data', 'accuracy on unseen data'])

# OPTIONAL
print("accuracy: {}".format(model.evaluate(test_X, test_y_ohe)[1]))

# REQUIRED
predictions = model.predict(test_X)
predictions = np.argmax(predictions, 1)
predictions_success = (predictions == test_y_numerical)

print(len(predictions_success[predictions_success == True]) / len(predictions_success))

test_y_actors = list((spectograms.get('speakers')[index] for index in test_y_index))
actor_predictin_tallies = np.zeros(shape=(len(np.unique(test_y_actors)), 2)) # 2D array. Row is actor. Col1 is correct predictions, col2 is total predictions

for index in range(len(predictions)):
  actor = test_y_actors[index]
  if predictions_success[index]:
    actor_predictin_tallies[int(actor) - 1001][0] += 1
  actor_predictin_tallies[int(actor) - 1001][1] += 1

actor_accuracies = actor_predictin_tallies[:,0] / actor_predictin_tallies[:,1]

plt.figure(figsize=(20,10))
plt.bar(x = range(len(actor_accuracies)), height=actor_accuracies, color=np.random.sample(size=(len(np.unique(test_y_actors)),3)))
plt.xlabel("Actor Index")
plt.ylabel("Accuracy")
plt.title("Accuracy of predicing each actor")

test_y_numerical
emotion_prediction_tallies = np.zeros(shape=(len(np.unique(test_y_numerical)), 2)) # 2D array. Row is actor. Col1 is correct predictions, col2 is total predictions

for index in range(len(predictions)):
  actor = test_y_numerical[index]
  if predictions_success[index]:
    emotion_prediction_tallies[test_y_numerical[index]][0] += 1
  emotion_prediction_tallies[test_y_numerical[index]][1] += 1

emotion_accuracies = emotion_prediction_tallies[:,0] / emotion_prediction_tallies[:,1]

plt.figure(figsize=(20,10))
plt.bar(x = EMOTIONS, height=emotion_accuracies, color=np.random.sample(size=(len(EMOTIONS),3)))
plt.xlabel("Emotion Index")
plt.ylabel("Accuracy")
plt.title("Accuracy of Predicting each Emotion")

"""# Training the Model on one Group of Speakers and Testing it on a Group of Unseen Speakers

Instead of testing on random samples from all users, split train and test sets such that different model will train and test on groups of totaly seperate speakers.

The datasets variables are unique to the random speker ones and can have preface ...
"""

# REQURIED

train_X = []
test_X = []
train_y_index = []
test_y_index = []

for spectogram_index in range(len(spectograms.get('clip_index'))):
  if int(spectograms.get('speakers')[spectogram_index]) < 1070:
    train_X.append(spectograms.get('log_mel_spectograms')[spectogram_index])
    train_y_index.append(spectograms.get('clip_index')[spectogram_index])
  else:
    test_X.append(spectograms.get('log_mel_spectograms')[spectogram_index])
    test_y_index.append(spectograms.get('clip_index')[spectogram_index])

train_X = prepare_x(train_X)
test_X = prepare_x(test_X)

# parallel to ..._y_index
train_y_ohe, train_y_numerical = prepare_y(train_y_index)
test_y_ohe, test_y_numerical = prepare_y(test_y_index)

# OPTIONAL

print("train_X shape: {}".format(train_X.shape))
print("test_X shape: {}".format(test_X.shape))
print("train_y shape: {}".format(train_y_ohe.shape))
print("test_y shape: {}".format(test_y_ohe.shape))

print("\nlabel -> numerical -> ohe")
for i in range(5):
  print("train_y index {}: {} -> {} -> {}".format(i,
                                                  y_classes_array[train_y_numerical[i]],
                                                  train_y_numerical[i],
                                                  train_y_ohe[i]
                                                  ))

print('\ntrain_y speaker | test_y speaker')
for i in range(10):
  print('     {0:10} | {1:10}'.format(spectograms.get('speakers')[train_y_index[i]],
                                 spectograms.get('speakers')[test_y_index[i]]))

# REQUIRED
model = get_new_model()
history = model.fit(train_X,
                    train_y_ohe,
                    epochs = epochs,
                    batch_size = batch_size,
                    validation_data=(test_X, test_y_ohe))

print("accuracy: {}".format(model.evaluate(test_X, test_y_ohe)[1]))